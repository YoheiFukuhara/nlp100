{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stanford Core NLPの共参照解析の結果に基づき，文中の参照表現（mention）を代表参照表現（representative mention）に置換せよ．ただし，置換するときは，「代表参照表現（参照表現）」のように，元の参照表現が分かるように配慮せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解析結果のxmlをパース\n",
    "root = ET.parse('./nlp.txt.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coreferenceの列挙し、代表参照表現に置き換える場所情報の辞書を作成\n",
    "#   辞書は{(sentence id, 開始token id), (終了token id, 代表参照表現)}...\n",
    "replaces = {}\n",
    "for coreference in root.iterfind('./document/coreference/coreference'):\n",
    "\n",
    "    # 代表参照表現の取得\n",
    "    representative = coreference.findtext('./mention[@representative=\"true\"]/text')\n",
    "\n",
    "    # 代表参照表現以外のmention列挙、辞書に追加\n",
    "    for mention in coreference.iterfind('./mention'):\n",
    "        if mention.get('representative') == None:\n",
    "            sentence_id = mention.findtext('sentence')\n",
    "            start = mention.findtext('start')\n",
    "            end = int(mention.findtext('end')) - 1 # endはずらす\n",
    "            \n",
    "            # すでに辞書にあっても気にせず更新(後勝ち)\n",
    "            replaces[(sentence_id, start)] = (end, representative)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural language processing From Wikipedia , the free encyclopedia Natural language processing -LRB- NLP -RRB- is a field of computer science) , artificial intelligence , and linguistics concerned with the interactions between computers and human -LRB- natural -RRB- languages . \n",
      "As such , 「NLP」 (NLP) is related to the area of humani-computer interaction . \n",
      "Many challenges in 「NLP」 (NLP) involve natural language understanding , that is , enabling computers to derive meaning from human or natural language input , and others involve natural language generation . \n",
      "History The history of 「NLP」 (NLP) generally starts in the 1950s , although work can be found from earlier periods . \n",
      "In 1950 , Alan Turing published an article titled `` Computing Machinery and Intelligence '' which proposed what is now called the Turing test as a criterion of intelligence . \n",
      "The Georgetown experiment in 1954 involved fully automatic translation of more than sixty Russian sentences into English . \n",
      "The authors claimed that within three or five years , machine translation would be a solved problem . \n",
      "However , real progress was much slower , and after the ALPAC report in 1966 , which found that ten year long research had failed to fulfill the expectations , funding for 「machine translation」 (machine translation) was dramatically reduced . \n",
      "Little further research in 「machine translation」 (machine translation) was conducted until the late 1980s , when the first statistical machine translation systems were developed . \n",
      "Some notably successful NLP systems developed in the 1960s were SHRDLU , a natural language system working in restricted `` blocks worlds '' with restricted vocabularies , and ELIZA , a simulation of a Rogerian psychotherapist , written by Joseph Weizenbaum between 1964 to 「1966」 (1966) . \n",
      "Using almost no information about human thought or emotion , ELIZA sometimes provided a startlingly human-like interaction . \n",
      "When the `` patient '' exceeded the very small knowledge base , 「ELIZA」 (ELIZA) might provide a generic response , for example , responding to `` My head hurts '' with `` Why do you say 「My head」 (your head) hurts ? '' \n",
      ". \n",
      "During the 1970s many programmers began to write ` conceptual ontologies ' , which structured real-world information into computer-understandable data . \n",
      "Examples are MARGIE -LRB- Schank , 1975 -RRB- , SAM -LRB- Cullingford , 1978 -RRB- , PAM -LRB- Wilensky , 「1978」 (1978) -RRB- , TaleSpin -LRB- Meehan , 1976 -RRB- , QUALM -LRB- Lehnert , 1977 -RRB- , Politics -LRB- Carbonell , 1979 -RRB- , and Plot Units -LRB- Lehnert 1981 -RRB- . \n",
      "During this time , many chatterbots were written including PARRY , Racter , and Jabberwacky . \n",
      "Up to the 1980s , most 「NLP」 (NLP) systems were based on complex sets of hand-written rules . \n",
      "Starting in the late 1980s , however , there was a revolution in 「NLP」 (NLP) with the introduction of machine learning algorithms for language processing . \n",
      "This was due to both the steady increase in computational power resulting from Moore 's Law and the gradual lessening of the dominance of Chomskyan theories of linguistics -LRB- e.g. transformational grammar -RRB- , whose theoretical underpinnings discouraged the sort of corpus linguistics that underlies the machine-learning approach to language processing . \n",
      "Some of the earliest-used machine learning algorithms , such as decision trees , produced systems of hard if-then rules similar to existing hand-written rules . \n",
      "However , Part of speech tagging introduced the use of Hidden Markov Models to NLP , and increasingly , research has focused on statistical models , which make soft , probabilistic decisions based on attaching real-valued weights to the features making up the input data . \n",
      "The cache language models upon which many speech recognition systems now rely are examples of such statistical models . \n",
      "Such models are generally more robust when given unfamiliar input , especially input that contains errors -LRB- as is very common for real-world data -RRB- , and produce more reliable results when integrated into a larger system comprising multiple subtasks . \n",
      "Many of the notable early successes occurred in the field of machine translation , due especially to work at IBM Research , where successively more complicated statistical models were developed . \n",
      "「many speech recognition systems」 (These systems) were able to take advantage of existing multilingual textual corpora that had been produced by the Parliament of Canada and the European Union as a result of laws calling for the translation of all governmental proceedings into all official languages of the corresponding systems of government . \n",
      "However , most other systems depended on corpora specifically developed for the tasks implemented by these systems , which was -LRB- and often continues to be -RRB- a major limitation in the success of 「many speech recognition systems」 (these systems) . \n",
      "As a result , a great deal of research has gone into methods of more effectively learning from limited amounts of data . \n",
      "Recent research has increasingly focused on unsupervised and semi-supervised learning algorithms . \n",
      "Such algorithms are able to learn from data that has not been hand-annotated with the desired answers , or using a combination of annotated and non-annotated data . \n",
      "Generally , this task is much more difficult than supervised learning , and typically produces less accurate results for a given amount of input data . \n",
      "However , there is an enormous amount of non-annotated data available -LRB- including , among other things , the entire content of the World Wide Web -RRB- , which can often make up for the inferior results . \n",
      "NLP using machine learning Modern NLP algorithms are based on machine learning , especially statistical machine learning . \n",
      "「The machine-learning paradigm」 (The paradigm of machine learning) is different from that of most prior attempts at language processing . \n",
      "Prior implementations of language-processing tasks typically involved the direct hand coding of large sets of rules . \n",
      "The machine-learning paradigm calls instead for using general learning algorithms - often , although not always , grounded in statistical inference - to automatically learn such rules through the analysis of large corpora of typical real-world examples . \n",
      "A corpus -LRB- plural , `` corpora '' -RRB- is a set of documents -LRB- or sometimes , individual sentences -RRB- that have been hand-annotated with the correct values to be learned . \n",
      "Many different classes of machine learning algorithms have been applied to 「NLP」 (NLP) tasks . \n",
      "「machine learning algorithms」 (These algorithms) take as input a large set of `` features '' that are generated from 「the input data」 (the input data) . \n",
      "Some of 「machine learning algorithms」 (the earliest-used algorithms) , such as decision trees , produced systems of hard if-then rules similar to the systems of hand-written rules that were then common . \n",
      "Increasingly , however , research has focused on statistical models , which make soft , probabilistic decisions based on attaching real-valued weights to each input feature . \n",
      "Such models have the advantage that 「Such models」 (they) can express the relative certainty of many different possible answers rather than only one , producing more reliable results when such a model is included as a component of a larger system . \n",
      "Systems based on machine-learning algorithms have many advantages over hand-produced rules : The learning procedures used during machine learning automatically focus on the most common cases , whereas when writing rules by hand it is often not obvious at all where the effort should be directed . \n",
      "Automatic learning procedures can make use of statistical inference algorithms to produce models that are robust to unfamiliar input -LRB- e.g. containing words or structures that have not been seen before -RRB- and to erroneous input -LRB- e.g. with misspelled words or words accidentally omitted -RRB- . \n",
      "Generally , handling such input gracefully with hand-written rules -- or more generally , creating systems of hand-written rules that make soft decisions -- extremely difficult , error-prone and time-consuming . \n",
      "Systems based on automatically learning 「hand-written rules --」 (the rules) can be made more accurate simply by supplying more input data . \n",
      "However , systems based on hand-written rules can only be made more accurate by increasing the complexity of the rules , which is a much more difficult task . \n",
      "In particular , there is a limit to the complexity of systems based on hand-crafted rules , beyond which 「many speech recognition systems」 (the systems) become more and more unmanageable . \n",
      "However , creating more data to input to machine-learning systems simply requires a corresponding increase in the number of man-hours worked , generally without significant increases in the complexity of the annotation process . \n",
      "The subfield of 「NLP」 (NLP) devoted to learning approaches is known as Natural Language Learning -LRB- NLL -RRB- and 「NLP」 (its) conference CoNLL and peak body SIGNLL are sponsored by ACL , recognizing also their links with Computational Linguistics and Language Acquisition . \n",
      "When the aims of computational language learning research is to understand more about human language acquisition , or psycholinguistics , 「NLL」 (NLL) overlaps into the related field of Computational Psycholinguistics . \n"
     ]
    }
   ],
   "source": [
    "# 本文をreplacesで置き換えながら表示\n",
    "for sentence in root.iterfind('./document/sentences/sentence'):\n",
    "    sentence_id = sentence.get('id')\n",
    "\n",
    "    for token in sentence.iterfind('./tokens/token'):\n",
    "        token_id = token.get('id')\n",
    "\n",
    "        # 置換スタート\n",
    "        if (sentence_id, token_id) in replaces:\n",
    "\n",
    "            # 辞書から終了位置と代表参照表現を取り出し\n",
    "            (end, representative) = replaces[(sentence_id, token_id)]\n",
    "\n",
    "            # 代表参照表現＋カッコを挿入(end=''で改行なし)\n",
    "            print('「' + representative + '」 (', end='')\n",
    "\n",
    "        # token出力(end=''で改行なし)\n",
    "        print(token.findtext('word'), end='')\n",
    "\n",
    "        # 置換の終わりなら閉じカッコを挿入(end=''で改行なし)\n",
    "        if int(token_id) == end:\n",
    "            print(')', end='')\n",
    "            end = 0\n",
    "            \n",
    "        # 文末(ピリオド)などの前にスペースが付加されることは気にしない(end=''で改行なし)\n",
    "        print(' ', end='')\n",
    "\n",
    "    print()     # sentence単位で改行"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
