{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stanford Core NLPの共参照解析の結果に基づき，文中の参照表現（mention）を代表参照表現（representative mention）に置換せよ．ただし，置換するときは，「代表参照表現（参照表現）」のように，元の参照表現が分かるように配慮せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = './nlp.txt'\n",
    "fname_parsed = './nlp.txt.xml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_nlp():\n",
    "    '''nlp.txtをStanford Core NLPで解析しxmlファイルへ出力\n",
    "    すでに結果ファイルが存在する場合は実行しない\n",
    "    '''\n",
    "    if not os.path.exists(fname_parsed):\n",
    "\n",
    "        # StanfordCoreNLP実行、標準エラーはparse.outへ出力\n",
    "        subprocess.run(\n",
    "            'java -cp \"/usr/local/lib/stanford-corenlp-full-2018-10-05/*\"'\n",
    "            ' -Xmx4g'\n",
    "            ' edu.stanford.nlp.pipeline.StanfordCoreNLP'\n",
    "            ' -annotators tokenize,ssplit,pos,lemma,ner,parse,dcoref'\n",
    "            ' -file ' + fname + ' 2>parse.out',\n",
    "            shell=True,     # shellで実行\n",
    "            check=True      # エラーチェックあり\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp.txtを解析\n",
    "parse_nlp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解析結果のxmlをパース\n",
    "root = ET.parse(fname_parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coreferenceの列挙し、代表参照表現に置き換える場所情報の辞書を作成\n",
    "#   辞書は{(sentence id, 開始token id), (終了token id, 代表参照表現)}...\n",
    "rep_dict = {}\n",
    "for coreference in root.iterfind('./document/coreference/coreference'):\n",
    "\n",
    "    # 代表参照表現の取得\n",
    "    rep_text = coreference.findtext('./mention[@representative=\"true\"]/text')\n",
    "\n",
    "    # 代表参照表現以外のmention列挙、辞書に追加\n",
    "    for mention in coreference.iterfind('./mention'):\n",
    "        if mention.get('representative', 'false') == 'false':\n",
    "\n",
    "            # 必要な情報の抽出\n",
    "            sent_id = int(mention.findtext('sentence'))\n",
    "            start = int(mention.findtext('start'))\n",
    "            end = int(mention.findtext('end'))\n",
    "\n",
    "            # すでに辞書にある（＝開始位置は同じだが終わりが違う）場合は先勝ち\n",
    "            if not (sent_id, start) in rep_dict:\n",
    "                rep_dict[(sent_id, start)] = (end, rep_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural language processing From Wikipedia , the free encyclopedia Natural language processing -LRB- NLP -RRB- is [the free encyclopedia Natural language processing -LRB- NLP -RRB-] (a field of computer science) , artificial intelligence , and linguistics concerned with the interactions between computers and human -LRB- natural -RRB- languages . \n",
      "As such , NLP is related to the area of humani-computer interaction . \n",
      "Many challenges in NLP involve natural language understanding , that is , enabling [computers] (computers) to derive meaning from human or natural language input , and others involve natural language generation . \n",
      "History The history of NLP generally starts in the 1950s , although work can be found from earlier periods . \n",
      "In 1950 , Alan Turing published an article titled `` Computing Machinery and Intelligence '' which proposed what is now called the [Alan Turing] (Turing) test as a criterion of intelligence . \n",
      "The Georgetown experiment in 1954 involved fully automatic translation of more than sixty Russian sentences into English . \n",
      "The authors claimed that within three or five years , [a solved problem] (machine translation) would be a solved problem . \n",
      "However , real progress was much slower , and after the ALPAC report in 1966 , which found that ten year long research had failed to fulfill the expectations , funding for machine translation was dramatically reduced . \n",
      "Little further research in [a solved problem] (machine translation) was conducted until the late 1980s , when the first statistical machine translation systems were developed . \n",
      "Some notably successful NLP systems developed in the 1960s were SHRDLU , [SHRDLU] (a natural language system working in restricted `` blocks worlds '' with restricted vocabularies) , and ELIZA , a simulation of a Rogerian psychotherapist , written by Joseph Weizenbaum between 1964 to 1966 . \n",
      "Using almost no information about human thought or emotion , ELIZA sometimes provided a startlingly human-like interaction . \n",
      "When the `` patient '' exceeded the very small knowledge base , [ELIZA] (ELIZA) might provide a generic response , for example , responding to `` [the `` patient ''] (My) head hurts '' with `` Why do you say [My head] (your head) hurts ? '' \n",
      ". \n",
      "During the 1970s many programmers began to write ` conceptual ontologies ' , which structured real-world information into computer-understandable data . \n",
      "Examples are MARGIE -LRB- Schank , 1975 -RRB- , SAM -LRB- Cullingford , 1978 -RRB- , PAM -LRB- Wilensky , [1978] (1978) -RRB- , TaleSpin -LRB- Meehan , 1976 -RRB- , QUALM -LRB- Lehnert , 1977 -RRB- , Politics -LRB- Carbonell , 1979 -RRB- , and Plot Units -LRB- Lehnert 1981 -RRB- . \n",
      "During this time , many chatterbots were written including PARRY , Racter , and Jabberwacky . \n",
      "Up to [the late 1980s] (the 1980s) , most [NLP] (NLP) systems were based on complex sets of hand-written rules . \n",
      "Starting in [the late 1980s] (the late 1980s) , however , there was a revolution in NLP with the introduction of machine learning algorithms for [the free encyclopedia Natural language processing -LRB- NLP -RRB-] (language processing) . \n",
      "This was due to both the steady increase in computational power resulting from Moore 's Law and the gradual lessening of the dominance of Chomskyan theories of linguistics -LRB- e.g. transformational grammar -RRB- , whose theoretical underpinnings discouraged the sort of corpus linguistics that underlies the machine-learning approach to [the free encyclopedia Natural language processing -LRB- NLP -RRB-] (language processing) . \n",
      "Some of the earliest-used machine learning [machine learning algorithms for language processing] (algorithms) , such as decision trees , produced systems of hard if-then rules similar to existing hand-written rules . \n",
      "However , Part of speech tagging introduced the use of Hidden Markov Models to [NLP] (NLP) , and increasingly , research has focused on [statistical models , which make soft , probabilistic decisions based on attaching real-valued weights to the features making up the input data] (statistical models) , which make soft , probabilistic decisions based on attaching real-valued weights to the features making up the input data . \n",
      "The cache language models upon which many speech recognition systems now rely are [The cache language models upon which many speech recognition systems now rely] (examples of such statistical models) . \n",
      "Such models are generally more robust when given unfamiliar input , especially [as input] (input that contains errors -LRB- as is very common for real-world data -RRB-) , and produce more reliable results when integrated into a larger system comprising multiple subtasks . \n",
      "Many of the notable early successes occurred in the field of [a solved problem] (machine translation , due especially to work at IBM Research , where successively more complicated statistical models were developed) . \n",
      "[many speech recognition systems] (These systems) were able to take [the advantage] (advantage of existing multilingual textual corpora that had been produced by the Parliament of Canada and the European Union as a result of laws calling for the translation of all governmental proceedings into all official languages of the corresponding systems of government) . \n",
      "However , most other systems depended on corpora specifically developed for the tasks implemented by [many speech recognition systems] (these systems) , which was -LRB- and often continues to be -RRB- a major limitation in the success of [many speech recognition systems] (these systems) . \n",
      "As a result , a great deal of research has gone into methods of more effectively learning from limited amounts of data . \n",
      "Recent research has increasingly focused on unsupervised and semi-supervised learning algorithms . \n",
      "Such algorithms are able to learn from [data] (data that has not been hand-annotated with the desired answers) , or using a combination of annotated and non-annotated data . \n",
      "Generally , this task is much more difficult than supervised learning , and typically produces less accurate results for a given amount of input data . \n",
      "However , there is an enormous amount of non-annotated data available -LRB- including , among other things , the entire content of the World Wide Web -RRB- , which can often make up for the inferior results . \n",
      "NLP using machine learning Modern NLP algorithms are based on [machine learning , especially statistical machine learning] (machine learning) , especially statistical machine learning . \n",
      "The paradigm of [machine learning , especially statistical machine learning] (machine learning) is different from that of most prior attempts at [the free encyclopedia Natural language processing -LRB- NLP -RRB-] (language processing) . \n",
      "Prior implementations of language-processing tasks typically involved the direct hand coding of large sets of [hard if-then rules similar to existing hand-written rules] (rules) . \n",
      "The machine-learning paradigm calls instead for using general learning algorithms - often , although not always , grounded in statistical inference - to automatically learn such rules through the analysis of large corpora of typical real-world examples . \n",
      "A corpus -LRB- plural , [existing multilingual textual corpora that had been produced by the Parliament of Canada and the European Union as a result of laws calling for the translation of all governmental proceedings into all official languages of the corresponding systems of government] (`` corpora '') -RRB- is [A corpus -LRB- plural , `` corpora '' -RRB-] (a set of documents -LRB- or sometimes , individual sentences -RRB- that have been hand-annotated with the correct values to be learned) . \n",
      "Many different classes of [machine learning Modern NLP algorithms] (machine learning algorithms) have been applied to NLP tasks . \n",
      "[machine learning Modern NLP algorithms] (These algorithms) take as input a large set of `` features '' that are generated from [the input data] (the input data) . \n",
      "Some of the earliest-used algorithms , such as [decision trees] (decision trees) , produced systems of hard if-then rules similar to the systems of [hand-written rules] (hand-written rules) that were then common . \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Increasingly , however , research has focused on [statistical models , which make soft , probabilistic decisions based on attaching real-valued weights to the features making up the input data] (statistical models , which make soft , probabilistic decisions based on attaching real-valued weights to each input feature) . \n",
      "Such models have the advantage that [Some of the earliest-used algorithms , such as decision trees] (they) can express the relative certainty of many different possible answers rather than only one , producing more reliable results when such a model is included as a component of a larger system . \n",
      "Systems based on machine-learning algorithms have many advantages over hand-produced rules : The learning procedures used during [machine learning , especially statistical machine learning] (machine learning) automatically focus on the most common cases , whereas when writing [hard if-then rules similar to existing hand-written rules] (rules) by hand it is often not obvious at all where the effort should be directed . \n",
      "Automatic learning [The learning procedures used during machine learning] (procedures) can make use of statistical inference algorithms to produce models that are robust to unfamiliar input -LRB- e.g. containing words or structures that have not been seen before -RRB- and to erroneous input -LRB- e.g. with misspelled words or words accidentally omitted -RRB- . \n",
      "Generally , handling such input gracefully with [hand-written rules] (hand-written rules) -- or more generally , creating systems of [hand-written rules] (hand-written rules) that make soft decisions -- extremely difficult , error-prone and time-consuming . \n",
      "Systems based on automatically learning the rules can be made more accurate simply by supplying more input data . \n",
      "However , [the systems] (systems based on hand-written rules) can only be made more accurate by increasing the complexity of [the rules] (the rules , which is a much more difficult task) . \n",
      "In particular , there is a limit to the complexity of systems based on hand-crafted rules , beyond which the systems become more and more unmanageable . \n",
      "However , creating more data to input to [Systems based on machine-learning algorithms] (machine-learning systems) simply requires a corresponding increase in the number of man-hours worked , generally without significant increases in the complexity of the annotation process . \n",
      "The subfield of NLP devoted to learning approaches is known as Natural Language Learning -LRB- NLL -RRB- and [Natural Language Learning -LRB- NLL -RRB-] (its) conference CoNLL and peak body SIGNLL are sponsored by ACL , recognizing also their links with Computational Linguistics and Language Acquisition . \n",
      "When the aims of computational language learning research is to understand more about human language acquisition , or psycholinguistics , NLL overlaps into the related field of Computational Psycholinguistics . \n"
     ]
    }
   ],
   "source": [
    "# 本文をrep_dictで置き換えながら表示\n",
    "for sentence in root.iterfind('./document/sentences/sentence'):\n",
    "    sent_id = int(sentence.get('id'))       # sentenceのid\n",
    "    org_rest = 0                            # 置換中のtoken数の残り\n",
    "\n",
    "    # token列挙\n",
    "    for token in sentence.iterfind('./tokens/token'):\n",
    "        token_id = int(token.get('id'))     # tokenのid\n",
    "\n",
    "        # 置換対象？\n",
    "        if org_rest == 0 and (sent_id, token_id) in rep_dict:\n",
    "\n",
    "            # 辞書から終了位置と代表参照表現を取り出し\n",
    "            (end, rep_text) = rep_dict[(sent_id, token_id)]\n",
    "\n",
    "            # 代表参照表現＋カッコを挿入\n",
    "            print('[' + rep_text + '] (', end='')\n",
    "            org_rest = end - token_id       # 置換中のtoken数の残り\n",
    "\n",
    "        # token出力\n",
    "        print(token.findtext('word'), end='')\n",
    "\n",
    "        # 置換の終わりなら閉じカッコを挿入\n",
    "        if org_rest > 0:\n",
    "            org_rest -= 1\n",
    "            if org_rest == 0:\n",
    "                print(')', end='')\n",
    "\n",
    "        print(' ', end='')\n",
    "\n",
    "    print()     # sentence単位で改行"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
