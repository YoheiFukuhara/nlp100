{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "極性分析に有用そうな素性を各自で設計し，学習データから素性を抽出せよ．素性としては，レビューからストップワードを除去し，各単語をステミング処理したものが最低限のベースラインとなるであろう．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from collections import Counter\n",
    "import csv\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import stanfordnlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 速くするためにタプルとして定義\n",
    "STOP_WORDS = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Universal POS tags に準拠していそう\n",
    "# https://universaldependencies.org/u/pos/\n",
    "EXC_POS = {'PUNCT',   # 句読点\n",
    "           'X',       # その他\n",
    "           'SYM',     # 記号\n",
    "           'PART',    # 助詞('sなど)\n",
    "           'NUM'}     # 番号"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use device: cpu\n",
      "---\n",
      "Loading: tokenize\n",
      "With settings: \n",
      "{'model_path': '/home/i348221/stanfordnlp_resources/en_ewt_models/en_ewt_tokenizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "---\n",
      "Loading: pos\n",
      "With settings: \n",
      "{'model_path': '/home/i348221/stanfordnlp_resources/en_ewt_models/en_ewt_tagger.pt', 'pretrain_path': '/home/i348221/stanfordnlp_resources/en_ewt_models/en_ewt.pretrain.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "---\n",
      "Loading: lemma\n",
      "With settings: \n",
      "{'model_path': '/home/i348221/stanfordnlp_resources/en_ewt_models/en_ewt_lemmatizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "Building an attentional Seq2Seq model...\n",
      "Using a Bi-LSTM encoder\n",
      "Using soft attention for LSTM.\n",
      "Finetune all embeddings.\n",
      "[Running seq2seq lemmatizer with edit classifier]\n",
      "Done loading processors!\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# プロセッサをデフォルトの全指定にするおｔ遅かったので最低限に絞る\n",
    "# https://stanfordnlp.github.io/stanfordnlp/processors.html\n",
    "nlp = stanfordnlp.Pipeline(processors='tokenize,pos,lemma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ストップワード真偽判定\n",
    "def is_stopword(word):\n",
    "    return True if word.lemma in STOP_WORDS \\\n",
    "                  or word.upos in EXC_POS \\\n",
    "                else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 警告非表示\n",
    "warnings.simplefilter('ignore', UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10661CPU times: user 1h 49min 54s, sys: 2min 57s, total: 1h 52min 52s\n",
      "Wall time: 45min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with open('./sentiment.txt') as file:\n",
    "    for i, line in enumerate(file):\n",
    "        print(\"\\r{0}\".format(i), end=\"\")\n",
    "        \n",
    "        # 最初の3文字はネガポジを示すだけなのでnlp処理しない(少しでも速くする)\n",
    "        doc = nlp(line[3:])\n",
    "        for sentence in doc.sentences:\n",
    "            lemma.extend([word.lemma for word in sentence.words if is_stopword(word) is False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_lemma = Counter(lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./lemma_all.txt', 'w') as f_out:\n",
    "    writer = csv.writer(f_out, delimiter='\\t')\n",
    "    writer.writerow(['Char', 'Freq'])\n",
    "    for key, value in freq_lemma.items():\n",
    "        writer.writerow([key] + [value])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
