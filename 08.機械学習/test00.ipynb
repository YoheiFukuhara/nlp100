{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk import stem\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.externals import joblib\n",
    " \n",
    "class Stopwords:\n",
    "    words = [\n",
    "        'a', 'about', 'all', 'an', 'and', 'any', 'are', 'as', \\\n",
    "        'at', 'be', 'been', 'but', 'by', 'can', 'could', 'do', \\\n",
    "        'does', 'for', 'from', 'has', 'have', 'he', 'her', 'his', \\\n",
    "        'how', 'i', 'if', 'in', 'into', 'is', 'it', 'its', 'made', \\\n",
    "        'make', 'may', 'me', 'my', 'no', 'not', 'of', 'on', 'one', \\\n",
    "        'or', 'out', 'she', 'should', 'so', 'some', 'than', 'that', \\\n",
    "        'the', 'their', 'them', 'there', 'then', 'they', 'this', \\\n",
    "        'those', 'to', 'too', 'us', 'was', 'we', 'what', 'when',\\\n",
    "        'which', 'who', 'with', 'would', 'you', 'your', ''\n",
    "    ]\n",
    " \n",
    "    @staticmethod\n",
    "    def exists(word):\n",
    "        return word in  Stopwords.words\n",
    " \n",
    "class SentimentFeatures:\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "        self.stemmer = stem.PorterStemmer()\n",
    " \n",
    "    @staticmethod\n",
    "    def isValid(word):\n",
    "        if word == '' or len(word) <= 2:\n",
    "            return False\n",
    "        if re.match(r'^[-=!@#$%^&*()_+|;\";,.<>/?]+$', word):\n",
    "            return False\n",
    "        return not Stopwords.exists(word)\n",
    " \n",
    "    def getFromLine(self, line):\n",
    "        sentiment = line[:3]\n",
    "        array = re.split(r'\\s|,|\\.|\\(|\\)|\\'|/|\\'|\\[|\\]|-', line[3:])\n",
    "        # こういう時はlambda キーワードいらないんですね。\n",
    "        words = filter(self.isValid, array)\n",
    "        xs = map(self.stemmer.stem, words)\n",
    "        return sentiment, xs\n",
    " \n",
    "    def enumerate(self):\n",
    "        with open(self.filename, 'r') as fin:\n",
    "            for line in fin:\n",
    "                yield self.getFromLine(line)\n",
    " \n",
    " \n",
    "class SentimentAnalyser:\n",
    "    def __init__(self):\n",
    "        self.cv = CountVectorizer(encoding='utf-8')\n",
    "        self.lr = LogisticRegression(solver='sag', max_iter=10000)\n",
    " \n",
    "    # LogisticRegression を使い学習する\n",
    "    def fit(self, X_train, y_train):\n",
    "        X_train_cv = self.cv.fit_transform(X_train)\n",
    "        self.lr.fit(X_train_cv, y_train)\n",
    "        print(self.lr.score(X_train_cv, y_train))\n",
    " \n",
    "    # 学習済みデータを保存する\n",
    "    def save(self):\n",
    "        # 学習したデータを保存する\n",
    "        joblib.dump(self.cv, './cv73.learn')\n",
    "        joblib.dump(self.lr, './lr73.learn')\n",
    " \n",
    "    def output(self):\n",
    "        print(self.lr.score)\n",
    "        return self.lr.score\n",
    "\n",
    "    # 学習に利用するデータを取り出す\n",
    "    # X[] は、素性データ\n",
    "    # y[] は、センチメント （正解データ)\n",
    "    @staticmethod\n",
    "    def getFeatureData(filename):\n",
    "        X = []\n",
    "        y = []\n",
    "        sf = SentimentFeatures(filename)\n",
    "        for sentiment, features in sf.enumerate():\n",
    "            y.append(1.0 if sentiment[0] == '+' else 0.0)\n",
    "            X.append(' '.join(features))\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9436315888201088\n"
     ]
    }
   ],
   "source": [
    "sa = SentimentAnalyser()\n",
    "X_train, y_train = sa.getFeatureData('./sentiment.txt')\n",
    "sa.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method ClassifierMixin.score of LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=10000,\n",
      "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='sag', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)>\n"
     ]
    }
   ],
   "source": [
    "lr = sa.output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.lancaster import LancasterStemmer as LS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = LS()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'he'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls.stem(\"He\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
