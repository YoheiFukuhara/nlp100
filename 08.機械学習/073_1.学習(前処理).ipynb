{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "極性分析に有用そうな素性を各自で設計し，学習データから素性を抽出せよ．素性としては，レビューからストップワードを除去し，各単語をステミング処理したものが最低限のベースラインとなるであろう．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import re\n",
    "import csv\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer as PS\n",
    "import stanfordnlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 速くするためにタプルとして定義\n",
    "STOP_WORDS = set(stopwords.words('english'))\n",
    "\n",
    "ps = PS()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Universal POS tags に準拠していそう\n",
    "# https://universaldependencies.org/u/pos/\n",
    "EXC_POS = {'PUNCT',   # 句読点\n",
    "           'X',       # その他\n",
    "           'SYM',     # 記号\n",
    "           'PART',    # 助詞('sなど)\n",
    "           'CCONJ',   # 接続詞(andなど)\n",
    "           'AUX',     # 助動詞(wouldなど)\n",
    "           'PRON',    # 代名詞\n",
    "           'SCONJ',   # 従位接続詞(whetherなど)\n",
    "           'ADP',     # 接置詞(inなど)\n",
    "           'NUM'}     # 番号"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use device: cpu\n",
      "---\n",
      "Loading: tokenize\n",
      "With settings: \n",
      "{'model_path': '/home/i348221/stanfordnlp_resources/en_ewt_models/en_ewt_tokenizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "---\n",
      "Loading: pos\n",
      "With settings: \n",
      "{'model_path': '/home/i348221/stanfordnlp_resources/en_ewt_models/en_ewt_tagger.pt', 'pretrain_path': '/home/i348221/stanfordnlp_resources/en_ewt_models/en_ewt.pretrain.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "---\n",
      "Loading: lemma\n",
      "With settings: \n",
      "{'model_path': '/home/i348221/stanfordnlp_resources/en_ewt_models/en_ewt_lemmatizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "Building an attentional Seq2Seq model...\n",
      "Using a Bi-LSTM encoder\n",
      "Using soft attention for LSTM.\n",
      "Finetune all embeddings.\n",
      "[Running seq2seq lemmatizer with edit classifier]\n",
      "Done loading processors!\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# プロセッサをデフォルトの全指定にするおｔ遅かったので最低限に絞る\n",
    "# https://stanfordnlp.github.io/stanfordnlp/processors.html\n",
    "nlp = stanfordnlp.Pipeline(processors='tokenize,pos,lemma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_sym = re.compile(r'^[!-/:-@[-`{-~]|[!-/:-@[-`{-~]$')\n",
    "reg_dit = re.compile('[0-9]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 先頭と末尾の記号除去\n",
    "def remove_symbols(lemma):\n",
    "    return ps.stem(reg_sym.sub('', lemma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ストップワード真偽判定\n",
    "def is_stopword(word):\n",
    "    lemma = remove_symbols(word.lemma)\n",
    "    return True if lemma in STOP_WORDS \\\n",
    "                  or lemma == '' \\\n",
    "                  or word.upos in EXC_POS \\\n",
    "                  or len(lemma) == 1 \\\n",
    "                  or reg_dit.search(lemma)\\\n",
    "                else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 警告非表示\n",
    "warnings.simplefilter('ignore', UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10661CPU times: user 1h 27min 7s, sys: 4min 4s, total: 1h 31min 11s\n",
      "Wall time: 39min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with open('./sentiment.txt') as file_in:\n",
    "    with open('./sentiment_stem.txt', 'w') as file_out:\n",
    "        writer = csv.writer(file_out, delimiter='\\t')\n",
    "        writer.writerow(['Lable', 'Lemmas'])\n",
    "\n",
    "        for i, line in enumerate(file_in):\n",
    "            print(\"\\r{0}\".format(i), end=\"\")\n",
    "        \n",
    "            lemma = []\n",
    "        \n",
    "            # 最初の3文字はネガポジを示すだけなのでnlp処理しない(少しでも速くする)\n",
    "            doc = nlp(line[3:])\n",
    "            for sentence in doc.sentences:\n",
    "                lemma.extend([remove_symbols(word.lemma) for word in sentence.words if is_stopword(word) is False])\n",
    "            writer.writerow([1 if line[0] == '+' else 0, ' '.join(lemma)])\n",
    "            \n",
    "#            if i == 10:\n",
    "#                break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
