{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "45のプログラムを改変し，述語と格パターンに続けて項（述語に係っている文節そのもの）をタブ区切り形式で出力せよ．45の仕様に加えて，以下の仕様を満たすようにせよ．\n",
    "\n",
    "項は述語に係っている文節の単語列とする（末尾の助詞を取り除く必要はない）\n",
    "述語に係る文節が複数あるときは，助詞と同一の基準・順序でスペース区切りで並べる\n",
    "「吾輩はここで始めて人間というものを見た」という例文（neko.txt.cabochaの8文目）を考える． この文は「始める」と「見る」の２つの動詞を含み，「始める」に係る文節は「ここで」，「見る」に係る文節は「吾輩は」と「ものを」と解析された場合は，次のような出力になるはずである．\n",
    "\n",
    "始める  で      ここで\n",
    "見る    は を   吾輩は ものを"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class Morph:\n",
    "    def __init__(self, surface, base, pos, pos1):\n",
    "        self.surface = surface # 表層形(surface)\n",
    "        self.base    = base    # 基本形(base)\n",
    "        self.pos     = pos     # 品詞(pos)\n",
    "        self.pos1    = pos1    # 品詞細分類1(pos1)\n",
    "        \n",
    "    def __str__(self):\n",
    "        '''オブジェクトの文字列表現'''\n",
    "        return 'surface[{}]\\tbase[{}]\\tpos[{}]\\tpos1[{}]'\\\n",
    "            .format(self.surface, self.base, self.pos, self.pos1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chunk:\n",
    "    def __init__(self):\n",
    "        '''初期化'''\n",
    "        self.morphs = []\n",
    "        self.srcs   = []   # 係り元文節インデックス番号のリスト\n",
    "        self.dst    = -1   # 係り先文節インデックス番号(初期値:-1, 係り先がない場合は-1のまま)\n",
    "        \n",
    "    def __str__(self):\n",
    "        '''オブジェクトの文字列表現'''\n",
    "        surface = ''\n",
    "        for morph in self.morphs:\n",
    "            surface += morph.surface\n",
    "        return '{}\\tsrcs{}\\tdst[{}]'.format(surface, self.srcs, self.dst)\n",
    "    \n",
    "    def output_surface_wo_pct(self):\n",
    "        surface = ''\n",
    "#        pos_ok  = False\n",
    "        for morph in self.morphs:\n",
    "#            if morph.pos == pos:\n",
    "#                pos_ok = True\n",
    "            if morph.pos != '記号':\n",
    "                surface += morph.surface\n",
    "                #print(pos_ok)\n",
    "#        if pos_ok == False:\n",
    "#            return ''\n",
    "#        else:\n",
    "        return surface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neko_lines():\n",
    "    with open('./neko.txt.cabocha3', encoding='utf-8') as neko_cabocha:\n",
    "    \n",
    "        chunks = dict()     # idxをkeyにChunkを格納\n",
    "        \n",
    "        for line in neko_cabocha:\n",
    "            if line[:3] == 'EOS':\n",
    "                \n",
    "                # Chunkのリストを返す\n",
    "                if len(chunks) > 0:\n",
    "\n",
    "                    # chunksをkeyでソートし、valueのみ取り出し\n",
    "                    sorted_tuple = sorted(chunks.items(), key=lambda x: x[0])\n",
    "                    yield list(zip(*sorted_tuple))[1]  #[1]がリストのvalue部分\n",
    "                    chunks.clear()\n",
    "\n",
    "                else:\n",
    "                    yield []\n",
    "                    \n",
    "            # 先頭が*の行は係り受け解析結果なので、Chunkを作成\n",
    "            elif line[0] == '*':\n",
    "\n",
    "                # Chunkのインデックス番号と係り先のインデックス番号取得\n",
    "                cols = re.split('\\s|D', line)\n",
    "                idx = int(cols[1]) # Chunkのインデックス番号\n",
    "                dst = int(cols[2]) # 係り先文節インデックス番号\n",
    "\n",
    "                # Chunkを生成（なければ）し、係り先のインデックス番号セット\n",
    "                if idx not in chunks:\n",
    "                    chunks[idx] = Chunk()\n",
    "                chunks[idx].dst = dst\n",
    "\n",
    "                # 係り先のChunkを生成（なければ）し、係り元インデックス番号追加\n",
    "                if dst != -1:\n",
    "                    if dst not in chunks:\n",
    "                        chunks[dst] = Chunk()\n",
    "                    chunks[dst].srcs.append(idx) # 係り元は複数あるのでappend\n",
    "                    \n",
    "            else:\n",
    "                \n",
    "                #タブとカンマで分割\n",
    "                cols = re.split('\\t|,', line)\n",
    "\n",
    "                chunks[idx].morphs.append(Morph(\n",
    "                        cols[0],    # 表層形(surface)\n",
    "                        cols[7],    # 基本形(base)\n",
    "                        cols[1],    # 品詞(pos)\n",
    "                        cols[2]     # 品詞細分類1(pos1)\n",
    "                    ))\n",
    "        #print('stop')\n",
    "        #raise StopIteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_verb_with_joshi(chunk):\n",
    "    # 動詞のみループ\n",
    "    for verb in chunk.morphs:\n",
    "        if verb.pos == '動詞':\n",
    "            joshi_list = []\n",
    "\n",
    "            # 係り元ループ\n",
    "            for src in chunk.srcs:\n",
    "                \n",
    "                last_joshi = ''\n",
    "                sentence = ''\n",
    "\n",
    "                # 係り元の助詞のみループ\n",
    "                for source in chunks[src].morphs:\n",
    "                    sentence = sentence + source.surface\n",
    "                    if source.pos == '助詞':\n",
    "                        last_joshi = source.base  #退避\n",
    "                \n",
    "                # 助詞があったら追加\n",
    "                if last_joshi != '':\n",
    "                    joshi_list.append([last_joshi, chunks[src].output_surface_wo_pct()])\n",
    "\n",
    "            # ファイル出力\n",
    "            if len(joshi_list) > 0:\n",
    "                joshi_list.sort()\n",
    "                out_file.write('{}\\t{} {}\\n'.format(verb.base, \n",
    "                                                    ' '.join([line[0] for line in joshi_list]),    #助詞\n",
    "                                                    ' '.join([line[1] for line in joshi_list])) )  #文節\n",
    "            \n",
    "            # 動詞は1形態素につき最左のみなので１つ目でbreak\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 結果ファイル作成\n",
    "with open('./046_result.txt', mode='w') as out_file:\n",
    "\n",
    "    # 1文ずつリスト作成\n",
    "    for chunks in neko_lines():\n",
    "\n",
    "        # 1文\n",
    "        for chunk in chunks:\n",
    "            # 動詞の各パターンを抽出して出力\n",
    "            output_verb_with_joshi(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
