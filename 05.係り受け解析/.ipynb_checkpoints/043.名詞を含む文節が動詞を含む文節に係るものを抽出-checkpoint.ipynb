{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "名詞を含む文節が，動詞を含む文節に係るとき，これらをタブ区切り形式で抽出せよ．ただし，句読点などの記号は出力しないようにせよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class Morph:\n",
    "    def __init__(self, surface, base, pos, pos1):\n",
    "        self.surface = surface # 表層形(surface)\n",
    "        self.base    = base    # 基本形(base)\n",
    "        self.pos     = pos     # 品詞(pos)\n",
    "        self.pos1    = pos1    # 品詞細分類1(pos1)\n",
    "        \n",
    "    def __str__(self):\n",
    "        '''オブジェクトの文字列表現'''\n",
    "        return 'surface[{}]\\tbase[{}]\\tpos[{}]\\tpos1[{}]'\\\n",
    "            .format(self.surface, self.base, self.pos, self.pos1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chunk:\n",
    "    def __init__(self):\n",
    "        '''初期化'''\n",
    "        self.morphs = []\n",
    "        self.srcs   = []   # 係り元文節インデックス番号のリスト\n",
    "        self.dst    = -1   # 係り先文節インデックス番号(初期値:-1, 係り先がない場合は-1のまま)\n",
    "        \n",
    "    def __str__(self):\n",
    "        '''オブジェクトの文字列表現'''\n",
    "        surface = ''\n",
    "        for morph in self.morphs:\n",
    "            surface += morph.surface\n",
    "        return '{}\\tsrcs{}\\tdst[{}]'.format(surface, self.srcs, self.dst)\n",
    "    \n",
    "    def output_surface_wo_pct(self, pos):\n",
    "        surface = ''\n",
    "        pos_ok  = False\n",
    "        for morph in self.morphs:\n",
    "            if morph.pos == pos:\n",
    "                pos_ok = True\n",
    "            if morph.pos != '記号':\n",
    "                surface += morph.surface\n",
    "                #print(pos_ok)\n",
    "        if pos_ok == False:\n",
    "            return ''\n",
    "        else:\n",
    "            return surface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neko_lines():\n",
    "    with open('./neko.txt.cabocha3', encoding='utf-8') as neko_cabocha:\n",
    "    \n",
    "        chunks = dict()     # idxをkeyにChunkを格納\n",
    "        \n",
    "        for line in neko_cabocha:\n",
    "            if line[:3] == 'EOS':\n",
    "                \n",
    "                # Chunkのリストを返す\n",
    "                if len(chunks) > 0:\n",
    "\n",
    "                    # chunksをkeyでソートし、valueのみ取り出し\n",
    "                    sorted_tuple = sorted(chunks.items(), key=lambda x: x[0])\n",
    "                    yield list(zip(*sorted_tuple))[1]  #[1]がリストのvalue部分\n",
    "                    chunks.clear()\n",
    "\n",
    "                else:\n",
    "                    yield []\n",
    "                    \n",
    "            # 先頭が*の行は係り受け解析結果なので、Chunkを作成\n",
    "            elif line[0] == '*':\n",
    "\n",
    "                # Chunkのインデックス番号と係り先のインデックス番号取得\n",
    "                cols = re.split('\\s|D', line)\n",
    "                idx = int(cols[1]) # Chunkのインデックス番号\n",
    "                dst = int(cols[2]) # 係り先文節インデックス番号\n",
    "\n",
    "                # Chunkを生成（なければ）し、係り先のインデックス番号セット\n",
    "                if idx not in chunks:\n",
    "                    chunks[idx] = Chunk()\n",
    "                chunks[idx].dst = dst\n",
    "\n",
    "                # 係り先のChunkを生成（なければ）し、係り元インデックス番号追加\n",
    "                if dst != -1:\n",
    "                    if dst not in chunks:\n",
    "                        chunks[dst] = Chunk()\n",
    "                    chunks[dst].srcs.append(idx) # 係り元は複数あるのでappend\n",
    "                    \n",
    "            else:\n",
    "                \n",
    "                #タブとカンマで分割\n",
    "                cols = re.split('\\t|,', line)\n",
    "\n",
    "                chunks[idx].morphs.append(Morph(\n",
    "                        cols[0],    # 表層形(surface)\n",
    "                        cols[7],    # 基本形(base)\n",
    "                        cols[1],    # 品詞(pos)\n",
    "                        cols[2]     # 品詞細分類1(pos1)\n",
    "                    ))\n",
    "\n",
    "        raise StopIteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5:どこで\t生れたか\n",
      "5:見当が\tつかぬ\n",
      "6:所で\t泣いて\n",
      "6:ニャーニャー\t泣いて\n",
      "6:いた事だけは\t記憶している\n",
      "7:吾輩は\t見た\n",
      "7:ここで\t始めて\n",
      "7:ものを\t見た\n",
      "8:あとで\t聞くと\n",
      "9:我々を\t捕えて\n"
     ]
    }
   ],
   "source": [
    "# 1文ずつリスト作成\n",
    "for i, chunks in enumerate(neko_lines()):\n",
    "    \n",
    "    if i == 10:\n",
    "        break\n",
    "    \n",
    "    for chunk in chunks:\n",
    "    \n",
    "        if chunk.dst != -1:\n",
    "    \n",
    "            # 記号を除いた表層形をチェック、空なら除外\n",
    "            src = chunk.output_surface_wo_pct('名詞')\n",
    "            dst = chunks[chunk.dst].output_surface_wo_pct('動詞')\n",
    "            if src != '' and dst != '':\n",
    "                print('{}:{}\\t{}'.format(i, src, dst))            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
